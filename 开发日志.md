## 2025/3/14

1.学习掼蛋规则 

2.文献调研



3.打算分为2组，一组做Agent，一组做强化学习。

### Agent组：

### 强化学习组：

- [x] 学习规则
- [x] 发牌程序 `give_cards.py`

代码可参考项目：https://github.com/LSTM-Kirigaya/egg-pancake.git

训练一个掼蛋AI是一个非常有挑战性的任务，因为掼蛋不仅需要对抗性策略，还需要合作性策略。以下是几种适合处理合作与对抗场景的神经网络模型和技术建议：

---

1. **深度强化学习（Deep Reinforcement Learning, DRL）**
深度强化学习是训练游戏AI的常用方法，特别适合掼蛋这种需要动态决策的游戏。以下是几种适合的DRL模型：
- **Deep Q-Network (DQN)**：适用于离散动作空间，可以通过Q值学习来优化策略。
- **Proximal Policy Optimization (PPO)**：适合连续或高维动作空间，稳定性较高，适合掼蛋中复杂的策略调整。
- **Multi-Agent Deep Deterministic Policy Gradient (MADDPG)**：专门为多智能体设计，可以同时处理合作与对抗，适合掼蛋中玩家之间的互动。

---

2. **生成对抗网络（Generative Adversarial Networks, GANs）**
虽然GANs主要用于生成任务，但其对抗性训练机制可以借鉴到掼蛋AI中：
- **Conditional GANs**：可以生成特定条件下的策略或牌型，帮助AI在特定情境下做出决策。
- **Adversarial Training**：通过模拟对手的行为，训练AI在对抗中优化策略。

---

3. **多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）**
掼蛋涉及多个玩家之间的合作与对抗，MARL是处理这类问题的理想选择：
- **Cooperative MARL**：通过共享奖励或策略，训练AI与队友合作。
- **Competitive MARL**：通过对抗性训练，提升AI在竞争中的表现。
- **Mixed Cooperative-Competitive MARL**：结合合作与对抗，适合掼蛋中复杂的交互场景。

---

4. **图神经网络（Graph Neural Networks, GNNs）**
掼蛋的牌型和玩家关系可以用图结构表示，GNNs适合处理这种结构化数据：
- **Graph Attention Networks (GATs)**：可以捕捉玩家之间的动态关系，优化合作策略。
- **Graph Convolutional Networks (GCNs)**：用于分析牌型和玩家行为的关联性。

---

5. **模仿学习（Imitation Learning）**
通过模仿人类玩家的策略，快速提升AI的表现：
- **Behavioral Cloning**：直接模仿人类玩家的决策。
- **Inverse Reinforcement Learning (IRL)**：通过观察人类行为，推断其潜在奖励函数，从而优化AI策略。

---

6. **混合模型**
结合多种技术的混合模型可能更适合掼蛋AI：
- **DRL + GNNs**：用GNNs处理牌型和玩家关系，用DRL优化策略。
- **MARL + Imitation Learning**：通过模仿学习初始化策略，再用MARL进行优化。

---

7. **对抗样本训练**
为了提高AI的鲁棒性，可以引入对抗样本训练：
- **Adversarial Examples**：通过生成对抗样本，训练AI在复杂或欺骗性情境下的表现。

---

8. **实践建议**
- **数据集**：收集大量掼蛋对局数据，包括牌型、玩家行为和胜负结果。
- **奖励设计**：设计合理的奖励函数，平衡合作与对抗的目标。
- **评估指标**：除了胜率，还可以评估AI的合作能力、策略多样性和鲁棒性。
