# guandan
 掼蛋强化学习

 ## 2025/3/30

### 强化学习组：

不再使用DQN（状态太多），转换为PPO+ Actor-Critic

 **PPO（Proximal Policy Optimization，近端策略优化）+ Actor-Critic（评论家-演员）算法**

PPO 是一种强化学习算法，它基于 **策略梯度（Policy Gradient）**，并通过 **裁剪（Clipping）** 使得训练更加稳定。  
而 **Actor-Critic（AC，演员-评论家）** 是强化学习的一种架构，结合了 **值函数（Critic，评论家）** 和 **策略梯度（Actor，演员）**，以提高学习效率。

---

 **1. PPO + Actor-Critic 结构**
PPO 结合了 **Actor-Critic**，其中：
- **Actor（策略网络）**：负责决策，给定状态后，输出一个策略分布 $\pi_{\theta}(a|s)$。
- **Critic（价值网络）**：负责评估，计算状态值 $\(V(s)\)$ 估计当前状态的长期收益。

训练目标：
1. **Actor 训练目标** → 让策略在不剧烈变化的情况下，提高预期回报
2. **Critic 训练目标** → 让 Critic 准确估计状态值 $\(V(s)\)$

---

 **2. PPO 核心思想**
在策略梯度方法中，我们希望优化 **策略**（Actor），但如果策略变化过大，可能会导致学习不稳定。因此，PPO 采用 **策略约束** 来限制更新步长：

 **1️⃣ 旧策略与新策略的比值**
PPO 通过计算 **新旧策略的比值** 来控制更新：
$\[
r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
\]$
- 如果 $\( r_t(\theta) > 1 \)$，说明新策略比旧策略更倾向选择这个动作
- 如果 $\( r_t(\theta) < 1 \)$，说明新策略减少了选择这个动作的概率

**2️⃣ 目标函数（裁剪损失）**
PPO 使用 **裁剪（Clipping）** 技术，防止策略变化过大：
$\[
L(\theta) = \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t \right)
\]$
其中：
- $\( A_t \)$ 是 **优势函数**，衡量当前动作比平均水平好多少
- $\( \epsilon \$) 是裁剪范围（通常设为 0.2）
- 这个函数的作用是：
  - 当 $\( r_t(\theta) \)$ 变化太大时，直接限制其梯度，避免训练崩溃

 **3️⃣ Critic 的目标**
Critic 通过最小化 **均方误差（MSE）** 来优化值函数：
$\[
L_{\text{critic}} = (V(s) - \text{target})^2
\]$
其中：
- **target** 一般用 **TD 目标**（Temporal Difference Target）：  
  $\[
  \text{target} = r + \gamma V(s')
  \]$
  这表示当前状态的价值等于即时奖励 + 折扣后的下一个状态价值。

---

 **3. PPO 训练流程**
1. **初始化** Actor（策略网络）和 Critic（值网络）
2. **采集数据**：让 AI **自我对局**，收集 **状态-动作-奖励**
3. **计算优势**：计算 $\( A_t = Q(s,a) - V(s) \)$ 作为训练信号
4. **更新 Actor（策略）**：使用裁剪损失优化
5. **更新 Critic（值网络）**：最小化均方误差
6. **重复训练，直到策略收敛**

---

**4. PPO 的优点**
✅ **稳定性强**：裁剪策略防止过度更新，减少策略崩溃  
✅ **样本利用率高**：PPO 可以 **重复使用样本** 进行训练，提高数据效率  
✅ **适用于高维控制问题**：能用于复杂决策任务（如掼蛋 AI）  

---

 **5. 为什么 PPO 适合训练掼蛋 AI？**
1. **对局复杂度高**：掼蛋是 **不完全信息博弈**，PPO 适用于决策场景  
2. **动作空间大**：每个回合 AI 需要选择一个合法的牌型，PPO 可以通过 **策略网络** 输出分布来选择合适的牌  
3. **长期规划**：AI 需要在 **多个回合** 内做出最优决策，而 Critic 网络可以帮助 AI 评估局势

 ## 2025/3/21

### Agent组：

![e8aa1fcc1e882866969500d8a43f9455_720](https://github.com/user-attachments/assets/52af1fd5-a624-4064-a36f-8939e23751b2)

![a0d43911d79d063d858809f461f2bce6](https://github.com/user-attachments/assets/3189db79-15d6-4187-8e76-a0f7147de099)

### 强化学习组：

虚拟对局中放弃`逢人配`规则、`接风`规则、`贡牌`规则、`某队升级到A后，再取得一次非一四名胜利赢得游戏`规则、`某队升级到A后，连续输或取得一四名胜利3次，降级`规则（写不出来🤡）

目前支持级牌升级规则、A的特殊用法

准备开始模型搭建，以上未支持的规则放在以后实战时学习。

模型选择：`DQN`/`PPO`/`Transformer`

**状态表示**

**使用 One-Hot 编码（或多热编码）来表示手牌和其他信息**

| 特征                  | 维度             | 说明                                                                 |
|-----------------------|------------------|----------------------------------------------------------------------|
| 当前玩家的手牌         | 108              | 54 张牌，每张牌是否在手（双份）                                       |
| 其他玩家的手牌         | 3              | 3 维，表示剩余手牌数量（归一化））                                   |
| 每个玩家最近动作       | 108 × 4          | 每个玩家的最近出牌                                                     |
| 其他玩家出的牌         | 108 × 3          | 记录已经打出的牌                                                       |
| 当前级牌               | 13               | 级牌 one-hot 表示                                                      |
| 最近 20 次动作         | 108 × 4 × 5      | 5 轮历史，每轮 4 玩家，每人 108 维                                 |
| 协作状态               | 3                | 标识与队友的配合程度                                                   |
| 压制状态               | 3                | 标识对敌人的打压情况                                                   |
| 辅助状态               | 3                | 标识是否有意给队友铺路                                                 |
|总维度                   | `3049`         |   |
